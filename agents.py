"""
Stock Analysis Agents
ì£¼ì‹ ë¶„ì„ ì—ì´ì „íŠ¸ - ë‰´ìŠ¤ ì¤‘ì‹¬ ì „ëµ

ì „ëµ ë³€ê²½:
- SEC ê³µì‹œëŠ” ë¶€ê°€ì  ì†ŒìŠ¤ë¡œ í™œìš© (ì ‘ê·¼ ì œì•½)
- Bloomberg, Reuters, WSJ, FT ë“± ê³µì‹ ë ¥ ìˆëŠ” ë‰´ìŠ¤ë¥¼ primaryë¡œ ì‚¬ìš©
- ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ë¶„ì„ ì œê³µ
- ì—ì´ì „íŠ¸ë³„ Evaluator íŒ¨í„´ìœ¼ë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€
"""

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv(usecwd=True), override=True)

import os
import json
import re
import requests
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import date, timedelta, datetime
from tavily import TavilyClient
from openai import OpenAI
import FinanceDataReader as fdr
from pdfminer.high_level import extract_text as pdf_extract_text
from typing import List, Dict, Any, Optional, Tuple
import time

# ============================================================================
# CONFIGURATION
# ============================================================================

AGENT_NAMES = {
    "FILING": "Business & News Analysis (ë‰´ìŠ¤ ì¤‘ì‹¬ ë¶„ì„)",
    "FUNDAMENTAL": "Fundamental (Finviz ì¬ë¬´/ë°¸ë¥˜ ë¶„ì„)",
    "TECHNICAL": "Technical (FDR ì°¨íŠ¸ í•´ì„)"
}

def _clean(s: str) -> str:
    return (s or "").strip().strip('"').strip("'").replace("\uFEFF", "").replace("\r", "").replace("\n", "")

# API ì´ˆê¸°í™”
API_KEY = _clean(os.getenv("OPENAI_API_KEY", ""))
if not API_KEY:
    raise RuntimeError("OPENAI_API_KEYê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_PROJECT = _clean(os.getenv("OPENAI_PROJECT", "")) or None
OPENAI = OpenAI(api_key=API_KEY, project=OPENAI_PROJECT)

TAVILY = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
SEC_UA = os.environ.get("SEC_USER_AGENT", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

FINVIZ_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Referer": "https://finviz.com/"
}

# ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
PREMIUM_SOURCES = {
    "bloomberg.com": 10,
    "reuters.com": 10,
    "wsj.com": 9,
    "ft.com": 9,
    "cnbc.com": 8,
    "finance.yahoo.com": 7,
    "marketwatch.com": 7,
    "seekingalpha.com": 6,
    "barrons.com": 8,
    "fool.com": 5
}

# ì°¨ë‹¨í•  ë„ë©”ì¸
BLOCKED = {
    "dictionary.com",
    "investopedia.com",
    "wikipedia.org",
    "reddit.com",
    "quora.com",
    "shiftcomm.com"
}

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def _domain(url: str) -> str:
    """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
    try:
        return urlparse(url).netloc.lower()
    except:
        return ""

def _get_source_priority(url: str) -> int:
    """ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
    domain = _domain(url)
    for premium_domain, score in PREMIUM_SOURCES.items():
        if premium_domain in domain:
            return score
    return 1  # ê¸°ë³¸ ì ìˆ˜

def _ensure_source(state: dict, new_sources: list):
    """ì¤‘ë³µ ì—†ì´ ì†ŒìŠ¤ ì¶”ê°€"""
    known = {s["url"] for s in state["sources"]}
    for s in new_sources:
        if s.get("url") and s["url"] not in known:
            state["sources"].append(s)

def _save_price_chart(df: pd.DataFrame, ticker: str, outdir="artifacts") -> str:
    """ì£¼ê°€ ì°¨íŠ¸ ì €ì¥"""
    os.makedirs(outdir, exist_ok=True)
    path = os.path.join(outdir, f"{ticker}_daily.png")
    plt.figure(figsize=(9, 4))
    close_col = "Close" if "Close" in df.columns else "close"
    plt.plot(df.index, df[close_col], color='blue', label='Close')
    
    # SMA ê³„ì‚° ë° í”Œë¡¯
    try:
        sma20 = df[close_col].rolling(window=20).mean()
        sma60 = df[close_col].rolling(window=60).mean()
        plt.plot(df.index, sma20, color='orange', linestyle='--', linewidth=1.5, label='SMA20', alpha=0.8)
        plt.plot(df.index, sma60, color='green', linestyle='--', linewidth=1.5, label='SMA60', alpha=0.8)
    except Exception as e:
        print(f"    âš ï¸  SMA calculation failed: {e}")
    plt.title(f"{ticker} Daily Close")
    plt.xlabel("Date")
    plt.ylabel("Close Price")
    plt.legend()
    plt.tight_layout()
    plt.savefig(path)
    plt.close()
    return path

def _binary_eval(text: str, needed_topics: list, sources: list) -> dict:
    """ë¶„ì„ í’ˆì§ˆ í‰ê°€"""
    covered = sum(1 for m in needed_topics if m in text)
    coverage_ok = (len(needed_topics) == 0) or (covered / len(needed_topics) >= 0.8)
    grounded_ok = len(sources) >= 1
    contradiction_ok = ("ëª¨ìˆœ" not in text and "ì˜¤ë¥˜" not in text)
    safety_style_ok = (len(text) > 300 and "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸" not in text)
    domains = {s.get("domain") for s in sources}
    sources_ok = (len(sources) >= 3 and len(domains) >= 2)
    
    return {
        "sufficient": coverage_ok and grounded_ok and contradiction_ok and safety_style_ok and sources_ok,
        "coverage_ok": coverage_ok,
        "grounded_ok": grounded_ok,
        "contradiction_ok": contradiction_ok,
        "safety_style_ok": safety_style_ok,
        "sources_ok": sources_ok,
        "missing_points": [m for m in needed_topics if m not in text],
        "fixes": []
    }

# ============================================================================
# DATE FILTERING EVALUATORS (NEW)
# ============================================================================

def extract_year_from_date(date_str: str) -> int | None:
    """ë‚ ì§œ ë¬¸ìì—´ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
    if not date_str:
        return None
    
    # ISO í˜•ì‹: 2024-10-23
    if '-' in date_str and len(date_str) >= 10:
        try:
            return int(date_str[:4])
        except:
            pass
    
    # ë‹¤ë¥¸ í˜•ì‹ë“¤
    patterns = [
        r'(\d{4})-\d{2}-\d{2}',  # 2024-10-23
        r'(\d{4})/\d{2}/\d{2}',  # 2024/10/23
        r'\b(202[0-9])\b',       # 2020~2029
    ]
    
    for pattern in patterns:
        match = re.search(pattern, date_str)
        if match:
            try:
                return int(match.group(1))
            except:
                pass
    
    return None


def extract_year_from_text(text: str) -> int | None:
    """URLì´ë‚˜ ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
    # URL íŒ¨í„´: .../2024/10/article-name
    patterns = [
        r'/(202[4-5])/',
        r'-(202[4-5])-',
        r'\b(202[4-5])\b'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            try:
                return int(match.group(1))
            except:
                pass
    
    return None


def evaluate_source_freshness(sources: list[dict], cutoff_year: int = 2024) -> tuple[int, list[str]]:
    """
    ì†ŒìŠ¤ì˜ ìµœì‹ ì„± í‰ê°€ - 2024ë…„ ì´í›„ ê¸°ì‚¬ë§Œ í—ˆìš©
    
    Returns:
        (score, old_sources): scoreëŠ” 0(ì¬ì‹œë„) ë˜ëŠ” 1(í†µê³¼), old_sourcesëŠ” ì˜¤ë˜ëœ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    old_sources = []
    
    for src in sources:
        date_str = src.get("date", "")
        url = src.get("url", "")
        title = src.get("title", "")
        
        # 1. ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±
        year = extract_year_from_date(date_str)
        
        # 2. URL/ì œëª©ì—ì„œë„ ì—°ë„ ì¶”ì¶œ ì‹œë„
        if not year or year < cutoff_year:
            year_from_text = extract_year_from_text(f"{url} {title}")
            if year_from_text:
                year = year_from_text
        
        # 3. ì—¬ì „íˆ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ê¸°ë¡
        if year and year < cutoff_year:
            old_sources.append({
                "title": title[:60],
                "date": date_str,
                "year": year,
                "url": url
            })
    
    if old_sources:
        return 0, old_sources
    
    return 1, []

# ============================================================================
# CONTENT HALLUCINATION EVALUATORS (NEW)
# ============================================================================

def evaluate_filing_content(
    business_text: str, 
    product_info: dict,
    openai_client,
    ticker: str
) -> tuple[int, list[str]]:
    """
    FILING ì—ì´ì „íŠ¸ ì¶œë ¥ í‰ê°€ - í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦
    
    Returns:
        (score, issues): scoreëŠ” 0(ì¬ì‹œë„) ë˜ëŠ” 1(í†µê³¼), issuesëŠ” ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
    """
    issues = []
    
    sys = f"""You are a fact-checker for investment reports. Analyze the following text for:

1. **Tense Errors**: Check if past/present/future tenses are used correctly
   - Events before 2024: past tense
   - Current events (2024-2025): present tense
   - Future events (2026+): future tense
   - Be lenient with financial reporting (e.g., "Q4 2024 reported..." is acceptable)

2. **Suspicious Model Names**: Flag model names that seem CLEARLY FABRICATED
   - ONLY flag: obviously fake names like "Stealth Edition", "Ultra Plus", "Performance Max", made-up suffixes
   - IGNORE: Korean/translated versions of real models (e.g., "ë¨¸ìŠ¤íƒ± ë§ˆí•˜-E" = valid translation of "Mustang Mach-E")
   - IGNORE: Official model names even if unusual (e.g., "Mach-E", "Lightning", "Maverick" are real Ford models)
   - Compare with known {ticker} product lineup, but be conservative

3. **Invalid Promotions**: Check promotion validity
   - Must have specific dates/periods showing it's active in 2024-2025
   - Must have concrete benefits (discount amounts, not vague statements)
   - Flag "0% financing" without context as ambiguous
   - Expired promotions (before 2024) should be flagged

4. **Duplicate Information**: Check for repeated information with slight variations

Return JSON:
{{
  "tense_errors": [list of specific examples with clear errors ONLY],
  "fake_models": [list of CLEARLY FABRICATED model names ONLY - exclude translations],
  "invalid_promos": [list of invalid promotions],
  "duplicates": [list of duplicate entries],
  "score": 0 or 1
}}

Score should be 1 if there are NO MAJOR issues. Minor issues (translations, acceptable tense variations) should not affect score.
Be CONSERVATIVE - only flag clear, obvious problems.
"""
    
    content = f"""Business Text:
{business_text[:3000]}

Product Info:
{json.dumps(product_info, ensure_ascii=False)[:2000]}
"""
    
    try:
        resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": sys},
                {"role": "user", "content": content}
            ],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        
        result = json.loads(resp.choices[0].message.content)
        
        if result.get("tense_errors"):
            issues.append(f"âŒ Tense errors: {len(result['tense_errors'])} found")
            for err in result["tense_errors"][:2]:
                issues.append(f"  â†’ {err}")
        
        if result.get("fake_models"):
            issues.append(f"âŒ Suspicious models: {len(result['fake_models'])} found")
            for model in result["fake_models"][:2]:
                issues.append(f"  â†’ {model}")
        
        if result.get("invalid_promos"):
            issues.append(f"âŒ Invalid promotions: {len(result['invalid_promos'])} found")
            for promo in result["invalid_promos"][:2]:
                issues.append(f"  â†’ {promo}")
        
        if result.get("duplicates"):
            issues.append(f"âš ï¸  Duplicate info: {len(result['duplicates'])} found")
        
        score = result.get("score", 0)
        
        return score, issues
        
    except Exception as e:
        print(f"âš ï¸ Content evaluation failed: {e}")
        return 1, []  # í‰ê°€ ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í†µê³¼


# ============================================================================
# ROUTING & PLANNING
# ============================================================================

def rewrite_query_for_routing(query: str) -> str:
    """ì¿¼ë¦¬ë¥¼ ë¼ìš°íŒ… ì¹œí™”ì ìœ¼ë¡œ ë¦¬ë¼ì´íŒ…"""
    try:
        resp = OPENAI.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.1,
            messages=[
                {"role": "system", "content":
                 "Rewrite the user's request into a compact, routing-friendly English query. "
                 "Include company/ticker if present, and a hint: value or technical. "
                 "Output one line only."},
                {"role": "user", "content": query}
            ]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸  Query rewrite failed: {e}")
        return query

def _lock_entity(query_or_rewritten: str) -> dict:
    """ì—”í‹°í‹°(íšŒì‚¬) ì •ë³´ ì¶”ì¶œ"""
    sys = "Extract company entity (name,ticker,exchange) from the text. JSON {entity:{name,ticker,exchange}}"
    try:
        resp = OPENAI.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.0,
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": query_or_rewritten}],
            response_format={"type": "json_object"}
        )
        ent = json.loads(resp.choices[0].message.content).get("entity", {})
    except:
        ent = {
            "name": query_or_rewritten.strip(),
            "ticker": query_or_rewritten.strip().upper(),
            "exchange": "UNKNOWN"
        }
    ent["confidence"] = 0.9
    return ent

def route_with_llm(query: str, rewritten_query: str | None = None, context: dict | None = None) -> dict:
    """LLM ê¸°ë°˜ ë¼ìš°íŒ… ê²°ì •"""
    ctx = context or {}
    forced_persp = ctx.get("forced_perspective")
    forced_ticker = ctx.get("forced_ticker")

    ent = _lock_entity(rewritten_query or query)
    if forced_ticker:
        ent["ticker"] = forced_ticker
        ent["confidence"] = 0.99

    system = (
        "Determine the investment perspective (value or tech) for the user's query and choose which agents to run.\n"
        "\n"
        "ROUTING RULES:\n"
        "1. ALWAYS include 'FILING' (news & business analysis)\n"
        "2. If query mentions 'value', 'fundamental', 'ê°€ì¹˜', 'ì¬ë¬´', 'ë°¸ë¥˜ì—ì´ì…˜':\n"
        "   â†’ Add 'FUNDAMENTAL' (financial analysis)\n"
        "3. If query mentions 'tech', 'technical', 'ê¸°ìˆ ì ', 'ì°¨íŠ¸', 'ì‹œì„¸':\n"
        "   â†’ Add 'TECHNICAL' (chart analysis)\n"
        "4. If unsure, default to perspective='value' and route=['FILING', 'FUNDAMENTAL']\n"
        "\n"
        "Return JSON: {\n"
        "  'perspective': 'value' or 'tech',\n"
        "  'route': ['FILING', 'FUNDAMENTAL'] or ['FILING', 'TECHNICAL'],\n"
        "  'must_cover': [list of topics],\n"
        "  'search_seeds': [list],\n"
        "  'entity': {name, ticker, exchange}\n"
        "}"
    )
    user = json.dumps(
        {"query": query, "rewritten": rewritten_query, "entity_hint": ent, "context": ctx},
        ensure_ascii=False,
    )
    
    try:
        resp = OPENAI.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.1,
            messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
            response_format={"type": "json_object"},
        )
        obj = json.loads(resp.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸  Routing failed: {e}")
        obj = {
            "perspective": "value",
            "route": ["FILING", "FUNDAMENTAL"],
            "must_cover": [
                "business structure & catalysts",
                "revenue comparison",
                "risk factors",
                "investment recommendation",
            ],
            "search_seeds": [],
            "entity": ent,
        }

    # ê°•ì œ ì„¤ì • ì ìš©
    if forced_persp in ("value", "tech"):
        obj["perspective"] = forced_persp
        obj["route"] = ["FILING", "FUNDAMENTAL"] if forced_persp == "value" else ["FILING", "TECHNICAL"]

    obj["entity"] = obj.get("entity") or ent

    # route ì •ê·œí™”
    route_list = obj.get("route", ["FILING", "FUNDAMENTAL"])
    if isinstance(route_list, str):
        route_list = [route_list]
    obj["route"] = route_list

    # must_cover ì •ê·œí™”
    must_cover = obj.get("must_cover", [])
    if isinstance(must_cover, str):
        must_cover = [must_cover]
    elif not isinstance(must_cover, list):
        must_cover = list(must_cover)
    obj["must_cover"] = must_cover + [
        "business structure & catalysts",
        "revenue comparison",
        "risk factors",
        "investment recommendation",
    ]

    # search_seeds ì •ê·œí™”
    search_seeds = obj.get("search_seeds", [])
    if isinstance(search_seeds, str):
        search_seeds = [search_seeds]
    elif not isinstance(search_seeds, list):
        search_seeds = list(search_seeds)
    obj["search_seeds"] = search_seeds

    return obj

def decompose_tasks_with_llm(route_obj: dict) -> dict:
    """ì‘ì—… ë¶„í•´ ê³„íš ìƒì„±"""
    sys = (
        "ë„ˆëŠ” ë¦¬ì„œì¹˜ í”Œë˜ë„ˆë‹¤. ë¼ìš°íŒ… ì •ë³´(FILING/FUNDAMENTAL/TECHNICAL)ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "ìµœì†Œ ë‹¨ê³„ì˜ ì‘ì—… ë¶„í•´ë¥¼ 4~8ê°œ ì œì•ˆí•˜ë¼. ê° ë‹¨ê³„ëŠ” agent ì½”ë“œ('FILING','FUNDAMENTAL','TECHNICAL')ì™€ í•œ ë¬¸ì¥ desc. "
        "ë§ˆì§€ë§‰ì— ì™„ë£Œ ê¸°ì¤€ 3~5ê°œ. JSONë§Œ."
    )
    user = json.dumps({
        "entity": route_obj.get("entity"),
        "perspective": route_obj.get("perspective"),
        "route": route_obj.get("route"),
        "must_cover": route_obj.get("must_cover")
    }, ensure_ascii=False)
    
    try:
        resp = OPENAI.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.1,
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": user}],
            response_format={"type": "json_object"}
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸  Task decomposition failed: {e}")
        route = route_obj.get("route", ["FILING", "FUNDAMENTAL"])
        steps = [
            {"id": "S1", "agent": "FILING", "desc": "ë‰´ìŠ¤ ë° ê³µì‹œì—ì„œ ì‚¬ì—… ë™í–¥ê³¼ ë§¤ì¶œ ë¹„êµ ì¶”ì¶œ"},
            {"id": "S2", "agent": "FUNDAMENTAL", "desc": "Finvizì—ì„œ ì¬ë¬´/ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ ìˆ˜ì§‘ ë° ìš”ì•½"} if "FUNDAMENTAL" in route else None,
            {"id": "S3", "agent": "TECHNICAL", "desc": "FinanceDataReaderë¡œ ì£¼ê°€ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ìƒì„±"} if "TECHNICAL" in route else None,
            {"id": "S4", "agent": "REPORT", "desc": "ì„¹ì…˜ í•©ì„± ë° íˆ¬ì ì¶”ì²œ(Buy/Hold/Sell) ìƒì„±"}
        ]
        steps = [s for s in steps if s is not None]
        acceptance = [
            "ì‚¬ì—… ì „ê°œ/ë™í–¥ ì„œìˆ  í¬í•¨",
            "ë¶„ê¸° ë§¤ì¶œ ë¹„êµ í‘œ í¬í•¨",
            "ì¶œì²˜ 3ê°œ ì´ìƒ ë° ë„ë©”ì¸ ë‹¤ì–‘ì„±",
            "ëª¨ìˆœ/ì˜¤ë¥˜ ì—†ìŒ",
            "íˆ¬ì ì¶”ì²œ(Buy/Hold/Sell) í¬í•¨"
        ]
        return {"steps": steps, "acceptance": acceptance}

# ============================================================================
# NEWS & DATA COLLECTION (ë‰´ìŠ¤ ì¤‘ì‹¬ ì „ëµ)
# ============================================================================

def fetch_comprehensive_news(ticker: str, max_results: int = 15, debug: bool = True) -> list[dict]:
    """
    ì¢…í•© ë‰´ìŠ¤ ìˆ˜ì§‘ - ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ í’ë¶€í•œ ì •ë³´ í™•ë³´
    
    ì „ëµ:
    1. ì‹¤ì /ì¬ë¬´ ë‰´ìŠ¤
    2. ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
    3. ìµœê·¼ ë™í–¥
    4. ê²½ìŸì‚¬/ì‚°ì—… ë™í–¥
    """
    if debug:
        print(f"[NEWS] ğŸ“° Comprehensive news search for {ticker}...")
    
    queries = [
        # ì‹¤ì  & ì¬ë¬´
        f"{ticker} latest earnings report revenue profit 2024 2025",
        f"{ticker} quarterly results financial performance 2024",
        
        # ì œí’ˆ & ëª¨ë¸ (ì—°ë„ ëª…ì‹œ!)
        f"{ticker} new models launch vehicles cars 2024 2025",
        f"{ticker} electric vehicle EV battery hybrid models 2024",
        f"{ticker} vehicle lineup product portfolio 2024",
        
        # ì• ë„ë¦¬ìŠ¤íŠ¸ & ë“±ê¸‰
        f"{ticker} analyst rating price target upgrade downgrade 2024",
        f"{ticker} stock recommendation buy sell hold 2024",
        
        # ìµœê·¼ ë™í–¥ & ì „ëµ
        f"{ticker} latest news updates announcement 2024 2025",
        f"{ticker} business development strategic initiative expansion 2024",
        f"{ticker} technology innovation autonomous software 2024",
        
        # ì‚°ì—… & ê²½ìŸ
        f"{ticker} industry trends market share competition 2024",
    ]
    
    all_articles = []
    seen_urls = set()
    successful_queries = 0
    
    for i, q in enumerate(queries, 1):
        try:
            if debug:
                print(f"[NEWS]   Query {i}/{len(queries)}: {q[:50]}...")
            
            # Tavily API ì†ë„ ì œí•œ íšŒí”¼
            if i > 1:
                time.sleep(1.5)
            
            results = TAVILY.search(
                query=q,
                max_results=max(2, max_results // len(queries)),
                search_depth="basic"
            )
            
            successful_queries += 1
            
            for it in results.get("results", []):
                url = it.get("url", "")
                title = it.get("title", "")
                
                if not url or not title or url in seen_urls:
                    continue
                
                domain = _domain(url)
                
                # ì°¨ë‹¨ ë„ë©”ì¸ í•„í„°ë§
                if domain in BLOCKED:
                    continue
                
                # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
                priority = _get_source_priority(url)
                
                article = {
                    "url": url,
                    "domain": domain,
                    "title": title,
                    "content": it.get("content", ""),
                    "type": "news",
                    "date": it.get("published_date") or str(date.today()),
                    "priority": priority,
                    "score": it.get("score", 0.0)
                }
                
                all_articles.append(article)
                seen_urls.add(url)
                
            # ì¡°ê¸° ì¢…ë£Œ
            if len(all_articles) >= max_results:
                if debug:
                    print(f"[NEWS]   âœ… Sufficient sources, stopping early")
                break
                
        except Exception as e:
            error_msg = str(e)
            if "excessive requests" in error_msg.lower() or "rate limit" in error_msg.lower():
                if debug:
                    print(f"[NEWS]   âš ï¸  Rate limit hit, waiting 5s...")
                time.sleep(5)
                try:
                    time.sleep(2)
                    results = TAVILY.search(query=q, max_results=2, search_depth="basic")
                    successful_queries += 1
                    for it in results.get("results", []):
                        url = it.get("url", "")
                        if url and url not in seen_urls and _domain(url) not in BLOCKED:
                            all_articles.append({
                                "url": url,
                                "domain": _domain(url),
                                "title": it.get("title", ""),
                                "content": it.get("content", ""),
                                "type": "news",
                                "date": it.get("published_date") or str(date.today()),
                                "priority": _get_source_priority(url),
                                "score": it.get("score", 0.0)
                            })
                            seen_urls.add(url)
                except:
                    pass
            else:
                if debug:
                    print(f"[NEWS]   âš ï¸  Query failed: {str(e)[:60]}")
            continue
    
    # ë‚ ì§œ ì¶”ì¶œ ë° íŒŒì‹±
    def parse_date(date_str):
        """ë‚ ì§œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì •ë ¬ í‚¤ ë°˜í™˜ (ìµœì‹ ìˆœ)"""
        try:
            # ISO í˜•ì‹ (2025-10-23)
            if '-' in date_str and len(date_str) >= 10:
                return date_str[:10]
            # ê¸°íƒ€ í˜•ì‹ì€ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
            return str(datetime.now().date())
        except:
            return "2000-01-01"  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë˜ëœ ë‚ ì§œ
    
    # ê° ê¸°ì‚¬ì— ì •ë ¬ìš© ë‚ ì§œ ì¶”ê°€
    for article in all_articles:
        article["sort_date"] = parse_date(article.get("date", ""))
    
    # ìš°ì„ ìˆœìœ„ + ë‚ ì§œ + ì ìˆ˜ë¡œ ì •ë ¬ (ë‚ ì§œê°€ ìµœì‹ ì¼ìˆ˜ë¡ ìš°ì„ )
    all_articles.sort(
        key=lambda x: (x["priority"], x["sort_date"], x["score"]), 
        reverse=True
    )
    
    # ìƒìœ„ ê²°ê³¼ ì„ íƒ
    top_articles = all_articles[:max_results]
    
    if debug:
        print(f"[NEWS] âœ… Retrieved {len(top_articles)} articles from {len(set(a['domain'] for a in top_articles))} sources")
        print(f"[NEWS]   Successful queries: {successful_queries}/{len(queries)}")
        if top_articles:
            print(f"[NEWS]   Top sources: {', '.join(list(set(a['domain'] for a in top_articles[:5]))[:3])}")
    
    return top_articles

def try_fetch_sec_filings(ticker: str, limit: int = 3) -> list[dict]:
    """
    SEC ê³µì‹œ ì‹œë„ (ë¶€ê°€ì  ì†ŒìŠ¤)
    
    ì‹¤íŒ¨í•´ë„ ê´œì°®ìŒ - ë‰´ìŠ¤ë¡œ ì¶©ë¶„íˆ ì»¤ë²„ ê°€ëŠ¥
    """
    print(f"[SEC] ğŸ“‹ Attempting SEC filings for {ticker} (optional)...")
    
    try:
        queries = [
            f"{ticker} 10-K annual report site:sec.gov",
            f"{ticker} 10-Q quarterly filing site:sec.gov",
        ]
        
        filings = []
        seen_urls = set()
        
        for q in queries:
            if len(filings) >= limit:
                break
                
            try:
                results = TAVILY.search(
                    query=q,
                    max_results=limit,
                    search_depth="basic"
                )
                
                for it in results.get("results", []):
                    url = it.get("url", "")
                    if "sec.gov" in url and url not in seen_urls:
                        filings.append({
                            "title": it.get("title", "SEC Filing"),
                            "url": url,
                            "date": it.get("published_date", ""),
                            "type": "filing",
                            "priority": 8  # SECëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„
                        })
                        seen_urls.add(url)
            except:
                continue
        
        if filings:
            print(f"[SEC] âœ… Retrieved {len(filings)} SEC filings")
        else:
            print(f"[SEC] â„¹ï¸  No SEC filings found (continuing with news)")
        
        return filings[:limit]
        
    except Exception as e:
        print(f"[SEC] â„¹ï¸  SEC access unavailable: {str(e)[:60]}")
        print(f"[SEC] â„¹ï¸  Relying on news sources instead")
        return []

def _fetch_text_with_retry(url: str, timeout: int = 30, max_retries: int = 2) -> str:
    """í…ìŠ¤íŠ¸ fetch with retry"""
    for attempt in range(max_retries):
        try:
            headers = {"User-Agent": SEC_UA}
            r = requests.get(url, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r.text[:50000]  # 50Kë¡œ ì œí•œ
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                return ""
    return ""

def extract_revenue_from_text(text: str, ticker: str, openai_client) -> dict:
    """í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¶œ ì •ë³´ ì¶”ì¶œ"""
    if not text:
        return {}
    
    sys = (
        f"Extract latest quarter revenue for {ticker}. "
        "CRITICAL: Return revenue as STRING with unit (B for billions, M for millions, K for thousands). "
        "Examples: '48.3B' (not 48.3), '500M' (not 500), '1.2B' (not 1.2). "
        "Output JSON: {"
        "  'quarter_label': str,  // e.g., 'Q4 2024'"
        "  'last_quarter': str,   // e.g., '48.3B'"
        "  'prev_quarter': str,   // e.g., '46.2B' (previous quarter)"
        "  'yoy': str,            // e.g., '44.1B' (year-over-year comparison)"
        "  'yoy_quarter_label': str  // e.g., 'Q4 2023'"
        "}. "
        "If revenue is in billions, use 'B'. If millions, use 'M'. "
        "Return empty dict if no data found."
    )
    user = json.dumps({"snippet": text[:8000]}, ensure_ascii=False)
    
    try:
        resp = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.0,
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": user}],
            response_format={"type": "json_object"}
        )
        result = json.loads(resp.choices[0].message.content)
        
        # ë‹¨ìœ„ê°€ ìˆëŠ” ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼
        def parse_revenue(rev_str):
            if not rev_str or not isinstance(rev_str, str):
                return None
            try:
                rev_str = rev_str.strip().upper()
                if 'B' in rev_str:
                    return float(rev_str.replace('B', '').strip()) * 1e9
                elif 'M' in rev_str:
                    return float(rev_str.replace('M', '').strip()) * 1e6
                elif 'K' in rev_str:
                    return float(rev_str.replace('K', '').strip()) * 1e3
                else:
                    # ë‹¨ìœ„ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ float ì‹œë„
                    return float(rev_str)
            except:
                return None
        
        # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
        if result:
            result['last_quarter'] = parse_revenue(result.get('last_quarter'))
            result['prev_quarter'] = parse_revenue(result.get('prev_quarter'))
            result['yoy'] = parse_revenue(result.get('yoy'))
        
        return result
    except Exception as e:
        return {}

def extract_key_developments(text: str, title: str, openai_client) -> list[str]:
    """ì£¼ìš” ì‚¬ì—… ê°œë°œì‚¬í•­ ì¶”ì¶œ (ìƒì„¸ ë²„ì „)"""
    if not text:
        return []
    
    sys = (
        "Extract 5-8 key business developments with SPECIFIC DETAILS. "
        "Focus on:\n"
        "1. New products/models (names, specs, launch dates)\n"
        "2. Strategic initiatives (partnerships, investments, expansions)\n"
        "3. Financial performance (revenue, profit, guidance)\n"
        "4. Market developments (competition, market share, trends)\n"
        "5. Technology updates (EV, autonomous, software)\n"
        "6. Production/capacity changes (factories, output)\n"
        "7. Executive/organizational changes\n"
        "8. Regulatory/policy impacts\n\n"
        "Output JSON: {'developments': [str]}. "
        "Each should be a DETAILED sentence in Korean with specific names, numbers, and dates when available."
    )
    user = json.dumps({"title": title, "content": text[:12000]}, ensure_ascii=False)
    
    try:
        resp = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.0,
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": user}],
            response_format={"type": "json_object"}
        )
        return json.loads(resp.choices[0].message.content).get("developments", [])
    except Exception as e:
        return []


def extract_product_portfolio(text: str, title: str, ticker: str, openai_client) -> dict:
    """ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ë° ì‹ ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ìë™ì°¨ ì‚°ì—… íŠ¹í™”)"""
    if not text:
        return {}
    
    sys = (
        f"Extract {ticker}'s product portfolio and new model information.\n\n"
        "CRITICAL - USE CORRECT TENSE:\n"
        "- Past events (2023 or earlier): past tense (e.g., 'launched in 2023')\n"
        "- Current events (2024-2025): present tense (e.g., 'is launching', 'offers')\n"
        "- Future events (2026+): future tense (e.g., 'will launch')\n"
        "- Promotions: ONLY include if CURRENTLY ACTIVE in 2024-2025. Exclude expired promotions.\n\n"
        "Focus on:\n"
        "1. Current popular models (names, segments, features)\n"
        "2. Upcoming new models (names, launch dates, specs)\n"
        "3. EV/Hybrid models (range, charging, battery)\n"
        "4. Technology features (autonomous, connectivity, software)\n"
        "5. Promotions/pricing updates (ONLY if currently active with specific benefits)\n\n"
        "CRITICAL for Promotions:\n"
        "- ONLY include if:\n"
        "  * Has specific dates showing it's active in 2024-2025\n"
        "  * Has concrete benefits (discount amount, free item, etc.)\n"
        "  * NOT vague statements like '0% financing available'\n"
        "- If no clearly active promotions found, return empty list\n\n"
        "Output JSON:\n"
        "{\n"
        "  'current_models': [{'name': str, 'segment': str, 'details': str}],\n"
        "  'upcoming_models': [{'name': str, 'launch_date': str, 'details': str}],\n"
        "  'ev_updates': [str],\n"
        "  'technology': [str],\n"
        "  'promotions': [str]\n"
        "}\n\n"
        "Use Korean. Include specific names and numbers. Be conservative - when in doubt, exclude questionable information."
    )
    user = json.dumps({"title": title, "content": text[:12000]}, ensure_ascii=False)
    
    try:
        resp = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0.0,
            messages=[{"role": "system", "content": sys}, {"role": "user", "content": user}],
            response_format={"type": "json_object"}
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as e:
        return {}

# ============================================================================
# FILING BUSINESS AGENT (ë‰´ìŠ¤ ì¤‘ì‹¬ ë¦¬íŒ©í† ë§ + EVALUATOR)
# ============================================================================


def _merge_duplicate_models(models: list) -> list:
    """ì¤‘ë³µëœ ëª¨ë¸ëª…ì„ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ë¡œ í•©ì¹¨"""
    if not models:
        return []
    
    merged = {}
    for model in models:
        # ëª¨ë¸ëª… ì¶”ì¶œ (ê´„í˜¸ ì•ê¹Œì§€)
        if isinstance(model, dict):
            name = model.get("name", "")
            details = model.get("details", "")
        else:
            # ë¬¸ìì—´ì¸ ê²½ìš°
            name = model.split("(")[0].strip() if "(" in model else model.split(":")[0].strip()
            details = model
        
        # ì •ê·œí™” (ê³µë°±, ëŒ€ì†Œë¬¸ì)
        normalized_name = name.lower().strip()
        
        if normalized_name in merged:
            # ì´ë¯¸ ìˆìœ¼ë©´ ìƒì„¸ ì •ë³´ ë³‘í•©
            if isinstance(model, dict):
                merged[normalized_name]["details"] += f" {details}"
            else:
                merged[normalized_name] += f" {model.split(':', 1)[-1].strip() if ':' in model else ''}"
        else:
            merged[normalized_name] = model if isinstance(model, dict) else model
    
    return list(merged.values())

def filing_business_agent(
    state: dict, 
    progress=lambda *_: None, 
    max_retries: int = 2
) -> dict:
    """
    Filing & Business Agent (ë‰´ìŠ¤ ì¤‘ì‹¬ + Evaluator íŒ¨í„´)
    
    ì „ëµ:
    1. ê³ í’ˆì§ˆ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ í’ë¶€í•œ ì •ë³´ ìˆ˜ì§‘
    2. ë‚ ì§œ í•„í„°ë§ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ë§Œ ì„ íƒ (2024ë…„ ì´í›„)
    3. í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ (ì‹œì œ, ëª¨ë¸ëª…, í”„ë¡œëª¨ì…˜)
    4. SEC ê³µì‹œëŠ” ë³´ì¡°ì ìœ¼ë¡œ í™œìš© (ì‹¤íŒ¨í•´ë„ OK)
    """
    e = state["entity"]
    ticker = e["ticker"]
    openai_client = state["openai_client"]
    
    retry_count = 0
    
    while retry_count <= max_retries:
        progress(f"  â””â”€ [FILING] ğŸ“° News collection (attempt {retry_count + 1}/{max_retries + 1})...")
        
        # Step 1: ì¢…í•© ë‰´ìŠ¤ ìˆ˜ì§‘ (PRIMARY)
        news_sources = fetch_comprehensive_news(ticker, max_results=15, debug=True)
        
        # Step 2: ë‚ ì§œ í•„í„°ë§ í‰ê°€ (2024ë…„ ì´í›„ë§Œ)
        date_score, old_sources = evaluate_source_freshness(news_sources, cutoff_year=2024)
        
        if date_score == 0 and old_sources:
            print(f"    âš ï¸  Found {len(old_sources)} old articles:")
            for src in old_sources[:3]:
                print(f"      ğŸš« {src['year']} - {src['title']}")
            
            # ì˜¤ë˜ëœ ì†ŒìŠ¤ ì œê±°
            news_sources = [
                src for src in news_sources 
                if extract_year_from_date(src.get("date", "")) >= 2024
                or extract_year_from_text(f"{src.get('url', '')} {src.get('title', '')}") >= 2024
            ]
            
            print(f"      â†’ Filtered to {len(news_sources)} recent articles")
        
        # Step 3: ì†ŒìŠ¤ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        if len(news_sources) < 5:
            if retry_count < max_retries:
                print(f"    âš ï¸  Insufficient recent sources ({len(news_sources)}), retrying...")
                retry_count += 1
                time.sleep(2)
                continue
            else:
                print(f"    âš ï¸  Proceeding with {len(news_sources)} sources (max retries reached)")
        
        _ensure_source(state, news_sources)
        print(f"    â†’ Collected {len(news_sources)} high-quality news articles")

        # Step 4: SEC ê³µì‹œ ì‹œë„ (OPTIONAL)
        progress("  â””â”€ [FILING] ğŸ“‹ SEC filings (optional)...")
        sec_filings = try_fetch_sec_filings(ticker, limit=3)
        _ensure_source(state, sec_filings)

        # ëª¨ë“  ì†ŒìŠ¤ í†µí•©
        all_sources = news_sources + sec_filings

        # Step 5: ì†ŒìŠ¤ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„ì„
        progress("  â””â”€ [FILING] ğŸ“„ Analyzing sources...")
        
        developments = []
        rev_candidates = []
        filings_table = []
        product_info = {
            "current_models": [],
            "upcoming_models": [],
            "ev_updates": [],
            "technology": [],
            "promotions": []
        }
        
        for i, src in enumerate(all_sources[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ìƒì„¸ ë¶„ì„
            print(f"    Processing {i}/10: [{src['domain']}] {src['title'][:40]}...")
            
            # ì´ë¯¸ contentê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ fetch
            text = src.get("content", "")
            if not text:
                text = _fetch_text_with_retry(src["url"])
            
            if not text:
                continue
            
            # ì£¼ìš” ê°œë°œì‚¬í•­ ì¶”ì¶œ (ë” ìƒì„¸í•˜ê²Œ)
            devs = extract_key_developments(text, src["title"], openai_client)
            if devs:
                print(f"      â†’ Found {len(devs)} developments")
                developments.extend(devs)
            
            # ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ì¶”ì¶œ (ì²« 3ê°œ ì†ŒìŠ¤ì—ì„œë§Œ)
            if i <= 3:
                portfolio = extract_product_portfolio(text, src["title"], ticker, openai_client)
                if portfolio:
                    print(f"      â†’ Extracting product portfolio...")
                    for key in product_info:
                        if key in portfolio and portfolio[key]:
                            product_info[key].extend(portfolio[key])
            
            # ë§¤ì¶œ ì •ë³´ ì¶”ì¶œ
            rev_data = extract_revenue_from_text(text, ticker, openai_client)
            if rev_data and rev_data.get("last_quarter"):
                rev_candidates.append(rev_data)
                print(f"      â†’ Found revenue data: {rev_data.get('quarter_label', 'N/A')}")
            
            # SEC ê³µì‹œ í…Œì´ë¸”
            if src["type"] == "filing" and "sec.gov" in src["url"]:
                filings_table.append({
                    "Form": src["title"],
                    "Date": src.get("date", ""),
                    "URL": src["url"]
                })

        # Step 6: ë§¤ì¶œ ë°ì´í„° ì„ íƒ (ê°€ì¥ ìµœê·¼ ê²ƒ)
        rev_best = {}
        if rev_candidates:
            rev_best = max(rev_candidates, key=lambda x: x.get("last_quarter", 0))
        
        state.setdefault("tables", {})["filings"] = filings_table
        
        # Step 7: ë¹„ì¦ˆë‹ˆìŠ¤ í…ìŠ¤íŠ¸ ìƒì„±
        def _fmt(v):
            try:
                return f"${v/1e9:.2f}B" if abs(v) >= 1e9 else (f"${v/1e6:.2f}M" if abs(v) >= 1e6 else f"${v:,.0f}")
            except:
                return "N/A"

        business_text = "### ì‚¬ì—… ì „ê°œ ë° ë™í–¥ ë¶„ì„\n\n"
        
        # ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ì„¹ì…˜
        if any(product_info.values()):
            business_text += "#### ì œí’ˆ ë° ì‚¬ì—… í˜„í™©\n\n"
            
            # í˜„ì¬ ì£¼ë ¥ ëª¨ë¸
            if product_info["current_models"]:
                business_text += "**í˜„ì¬ ì£¼ë ¥ ëª¨ë¸**:\n"
                merged_models = _merge_duplicate_models(product_info["current_models"])
                for model in merged_models[:5]:
                    if isinstance(model, dict):
                        name = model.get("name", "")
                        segment = model.get("segment", "")
                        details = model.get("details", "")
                        business_text += f"- **{name}** ({segment}): {details}\n"
                    else:
                        business_text += f"- {model}\n"
                business_text += "\n"
            
            # ì‹ ëª¨ë¸ ë° ì¶œì‹œ ì˜ˆì •
            if product_info["upcoming_models"]:
                business_text += "**ì‹ ëª¨ë¸ ë° ì¶œì‹œ ì˜ˆì •**:\n"
                for model in product_info["upcoming_models"][:5]:
                    if isinstance(model, dict):
                        name = model.get("name", "")
                        launch = model.get("launch_date", "")
                        details = model.get("details", "")
                        launch_str = f" (ì¶œì‹œ: {launch})" if launch else ""
                        business_text += f"- **{name}**{launch_str}: {details}\n"
                    else:
                        business_text += f"- {model}\n"
                business_text += "\n"
            
            # ì „ê¸°ì°¨ ë™í–¥
            if product_info["ev_updates"]:
                business_text += "**ì „ê¸°ì°¨(EV) ë™í–¥**:\n"
                unique_ev = list(dict.fromkeys(product_info["ev_updates"]))[:6]
                for update in unique_ev:
                    business_text += f"- {update}\n"
                business_text += "\n"
            
            # ê¸°ìˆ  ë° í˜ì‹ 
            if product_info["technology"]:
                business_text += "**ê¸°ìˆ  ë° í˜ì‹ **:\n"
                for tech in product_info["technology"][:4]:
                    business_text += f"- {tech}\n"
                business_text += "\n"
            
            # í”„ë¡œëª¨ì…˜ (ëª…í™•í•œ í˜œíƒì´ ìˆëŠ” ê²ƒë§Œ)
            if product_info["promotions"]:
                business_text += "**í”„ë¡œëª¨ì…˜ ë° ê°€ê²© ì •ì±…**:\n"
                for promo in product_info["promotions"][:3]:
                    business_text += f"- {promo}\n"
                business_text += "\n"
        
        # ë§¤ì¶œ ì •ë³´
        if rev_best:
            business_text += "#### ì¬ë¬´ ì„±ê³¼\n\n"
            quarter_label = rev_best.get("quarter_label", "ìµœê·¼ ë¶„ê¸°")
            lq = rev_best.get("last_quarter")
            pq = rev_best.get("prev_quarter")
            yoy = rev_best.get("yoy")
            
            comp = "ì „ë…„ ë™ë¶„ê¸° ëŒ€ë¹„" if yoy else "ì „ë¶„ê¸° ëŒ€ë¹„"
            try:
                percentage = ((lq - yoy) / yoy * 100) if yoy else ((lq - pq) / pq * 100)
                business_text += f"**ë§¤ì¶œ í˜„í™©**: {quarter_label} ë§¤ì¶œì€ {_fmt(lq)}ë¡œ, {comp} {percentage:.1f}% ë³€í™”ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.\n\n"
            except:
                business_text += f"**ë§¤ì¶œ í˜„í™©**: {quarter_label} ë§¤ì¶œì€ {_fmt(lq)}ë¡œ í™•ì¸ë©ë‹ˆë‹¤.\n\n"
            
            state["tables"]["rev_compare"] = [{
                "Quarter": quarter_label,
                "Last": lq,
                "Prev/YoY": pq or yoy,
                "Type": "QoQ" if pq else "YoY"
            }]
        else:
            business_text += "#### ì¬ë¬´ ì„±ê³¼\n\n"
            business_text += "**ë§¤ì¶œ í˜„í™©**: ìµœê·¼ ë¶„ê¸° ë§¤ì¶œ ë°ì´í„°ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.\n\n"
            state["tables"]["rev_compare"] = []
        
        # ì£¼ìš” ê°œë°œì‚¬í•­ ë° ì‹œì¥ ë™í–¥
        if developments:
            business_text += "#### ì£¼ìš” ì‚¬ì—… ë™í–¥\n\n"
            unique_devs = list(dict.fromkeys(developments))[:10]
            for dev in unique_devs:
                business_text += f"- {dev}\n"
            business_text += "\n"
        
        # ì†ŒìŠ¤ ìš”ì•½
        business_text += f"**ë¶„ì„ ê¸°ë°˜**: {len(all_sources)}ê°œ ì†ŒìŠ¤ (ë‰´ìŠ¤ {len(news_sources)}ê°œ"
        if sec_filings:
            business_text += f", SEC ê³µì‹œ {len(sec_filings)}ê°œ"
        business_text += ")\n"

        # Step 8: ë‚´ìš© í‰ê°€ (í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬)
        progress("  â””â”€ [FILING] ğŸ” Content validation...")
        content_score, content_issues = evaluate_filing_content(
            business_text,
            product_info,
            openai_client,
            ticker
        )
        
        if content_score == 1:
            print(f"    âœ… FILING content validation passed")
            break
        else:
            print(f"    âš ï¸  Content issues found (attempt {retry_count + 1}/{max_retries + 1}):")
            for issue in content_issues:
                print(f"      {issue}")
            
            retry_count += 1
            if retry_count > max_retries:
                print(f"    âš ï¸  Max retries reached, proceeding with current content")
                print(f"    â„¹ï¸  User should review the report carefully for accuracy")
                break
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            time.sleep(2)

    # Step 9: ë¦¬ìŠ¤í¬ ìš”ì•½ ìƒì„±
    progress("  â””â”€ [FILING] âš ï¸  Risk analysis...")
    
    sys_risk = f"Based on {ticker} recent news and filings, summarize 3-5 key investment risks in Korean bullets. Focus on {state['perspective']} perspective."
    user_risk = json.dumps({
        "sources": [s["title"] for s in all_sources[:10]],
        "perspective": state["perspective"]
    }, ensure_ascii=False)
    
    try:
        resp_risk = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "system", "content": sys_risk}, {"role": "user", "content": user_risk}]
        )
        risks = "### ë¦¬ìŠ¤í¬ ë¶„ì„\n\n" + resp_risk.choices[0].message.content
    except Exception as e:
        print(f"    âš ï¸  Risk summary failed: {e}")
        risks = "### ë¦¬ìŠ¤í¬ ë¶„ì„\n\n- ì‚°ì—… ë³€ë™ì„±: ì‹œì¥ í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬\n- ê²½ìŸ ì‹¬í™”: ì—…ê³„ ê²½ìŸ ìƒí™© ëª¨ë‹ˆí„°ë§ í•„ìš”\n- ê·œì œ ë³€í™”: ì •ì±… ë° ê·œì œ ë¦¬ìŠ¤í¬\n"

    state["sections"]["business"] = business_text
    state["sections"]["risks"] = risks

    # Step 10: í‰ê°€
    needed = ["ì‚¬ì—… ì „ê°œ", "ë§¤ì¶œ", "ë¦¬ìŠ¤í¬", "íˆ¬ì ì¶”ì²œ"]
    eval_res = _binary_eval(business_text + "\n" + risks, needed, state["sources"])
    eval_res["retries"] = retry_count
    eval_res["date_filtering_passed"] = date_score == 1
    eval_res["content_validation_passed"] = content_score == 1
    state["evals"].append({"agent": "FILING", **eval_res})
    
    print(f"    âœ… FILING completed with {len(state['sources'])} total sources")
    print(f"       Retries: {retry_count}, Date Filter: {'âœ…' if date_score == 1 else 'âš ï¸'}, Content Valid: {'âœ…' if content_score == 1 else 'âš ï¸'}")
    
    return state

# ============================================================================
# FUNDAMENTAL AGENT
# ============================================================================

def _fetch_finviz_metrics(ticker: str) -> dict:
    """Finvizì—ì„œ ì¬ë¬´ ì§€í‘œ ìˆ˜ì§‘"""
    url = f"https://finviz.com/quote.ashx?t={ticker}"
    try:
        r = requests.get(url, headers=FINVIZ_HEADERS, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        table = soup.find("table", class_="snapshot-table2")
        if not table:
            return {"raw": {}}
        
        cells = [c.get_text(strip=True) for c in table.select("td")]
        kv = {}
        for i in range(0, len(cells)-1, 2):
            kv[cells[i]] = cells[i+1]
        
        def g(k): return kv.get(k, "N/A")
        
        return {"raw": {
            "MarketCap": g("Market Cap"),
            "P/E": g("P/E"),
            "Forward P/E": g("Forward P/E"),
            "PEG": g("PEG"),
            "P/S": g("P/S"),
            "P/B": g("P/B"),
            "EV/EBITDA": g("EV/EBITDA"),
            "ProfitMargin(%)": g("Profit Margin"),
            "OperMargin(%)": g("Oper. Margin"),
            "ROE(%)": g("ROE"),
            "Debt/Eq": g("Debt/Eq")
        }}
    except Exception as e:
        print(f"âš ï¸  Finviz fetch failed: {e}")
        return {"raw": {}}

def fundamental_agent(state: dict, progress=lambda *_: None) -> dict:
    """Fundamental Analysis Agent"""
    e = state["entity"]
    ticker = e["ticker"]
    
    progress("  â””â”€ [FUNDAMENTAL] ğŸ“Š Finviz metrics collection...")
    finviz = _fetch_finviz_metrics(ticker)
    
    blocks = {
        "valuation": {
            k: finviz["raw"].get(k, "N/A") 
            for k in ["P/E", "Forward P/E", "PEG", "P/S", "P/B", "EV/EBITDA"]
        },
        "profit_leverage": {
            k: finviz["raw"].get(k, "N/A") 
            for k in ["ProfitMargin(%)", "OperMargin(%)", "ROE(%)", "Debt/Eq"]
        }
    }
    
    state.setdefault("tables", {})["finviz_blocks"] = blocks

    val_txt = (
        "### ê°€ì¹˜íˆ¬ì ë¶„ì„\n\n"
        "**ë°¸ë¥˜ì—ì´ì…˜ ë©€í‹°í”Œ**\n"
        f"- P/E: {blocks['valuation'].get('P/E', 'N/A')} "
        f"(Forward P/E: {blocks['valuation'].get('Forward P/E', 'N/A')})\n"
        f"- PEG: {blocks['valuation'].get('PEG', 'N/A')}, "
        f"P/S: {blocks['valuation'].get('P/S', 'N/A')}, "
        f"P/B: {blocks['valuation'].get('P/B', 'N/A')}\n"
        f"- EV/EBITDA: {blocks['valuation'].get('EV/EBITDA', 'N/A')}\n\n"
        "**ìˆ˜ìµì„± ì§€í‘œ**\n"
        f"- Profit Margin: {blocks['profit_leverage'].get('ProfitMargin(%)', 'N/A')}, "
        f"Oper. Margin: {blocks['profit_leverage'].get('OperMargin(%)', 'N/A')}\n"
        f"- ROE: {blocks['profit_leverage'].get('ROE(%)', 'N/A')}\n\n"
        "**ì¬ë¬´ ê±´ì „ì„±**\n"
        f"- Debt/Eq: {blocks['profit_leverage'].get('Debt/Eq', 'N/A')}\n"
    )

    
    state["sections"]["value"] = val_txt
    
    _ensure_source(state, [{
        "url": f"https://finviz.com/quote.ashx?t={ticker}",
        "domain": "finviz.com",
        "title": f"Finviz Financial Snapshot - {ticker}",
        "type": "stats",
        "date": str(date.today()),
        "priority": 7
    }])
    
    state["evals"].append({
        "agent": "FUNDAMENTAL",
        **_binary_eval(val_txt, ["ë°¸ë¥˜ì—ì´ì…˜", "ì¬ë¬´"], state["sources"]),
        "retries": 0
    })
    
    print(f"    âœ… FUNDAMENTAL completed")
    return state

# ============================================================================
# TECHNICAL AGENT
# ============================================================================

def sma(series: pd.Series, window: int) -> pd.Series:
    return series.rolling(window).mean()

def rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = delta.clip(lower=0).ewm(alpha=1/period, adjust=False).mean()
    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean()
    rs = up / down.replace(0, np.nan)
    return 100 - (100 / (1 + rs))

def macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
    ema_fast = series.ewm(span=fast, adjust=False).mean()
    ema_slow = series.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    return macd_line, signal_line, macd_line - signal_line

def technical_agent(state: dict, progress=lambda *_: None) -> dict:
    """Technical Analysis Agent"""
    e = state["entity"]
    ticker = e["ticker"]
    
    progress("  â””â”€ [TECHNICAL] ğŸ“ˆ Chart analysis with FDR...")
    
    try:
        df = fdr.DataReader(ticker, start=date.today() - timedelta(days=365))
        state["artifacts"]["price_chart"] = _save_price_chart(df, ticker)
        
        close = df["Close"]
        rsi_v = rsi(close).iloc[-1]
        macd_line, signal_line, hist = macd(close)
        sma20 = sma(close, 20).iloc[-1]
        sma60 = sma(close, 60).iloc[-1]
        
        recent = close.tail(60)
        support = float(recent.min())
        resistance = float(recent.max())
        
        state.setdefault("tables", {})["technicals"] = {
            "RSI(14)": float(rsi_v),
            "SMA20": float(sma20),
            "SMA60": float(sma60)
        }
        state["tables"]["trend"] = {
            "Support (60d)": support,
            "Resistance (60d)": resistance
        }
        
        price = float(close.iloc[-1])
        
        lines = []
        
        # ì´ë™í‰ê·  ë¶„ì„
        if pd.notna(sma20) and pd.notna(sma60):
            if sma20 > sma60:
                lines.append("**ë‹¨ê¸° ì´ë™í‰ê· (20ì¼)ì´ ì¤‘ê¸°(60ì¼)ë³´ë‹¤ ë†’ì•„** ë‹¨ê¸° ìƒìŠ¹ ì¶”ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
            else:
                lines.append("**ë‹¨ê¸° ì´ë™í‰ê· (20ì¼)ì´ ì¤‘ê¸°(60ì¼)ë³´ë‹¤ ë‚®ì•„** ì¡°ì • êµ­ë©´ì— ìˆìŠµë‹ˆë‹¤.")
        
        # RSI ë¶„ì„
        if pd.notna(rsi_v):
            if rsi_v >= 70:
                lines.append(f"**RSI {rsi_v:.1f}**: ê³¼ë§¤ìˆ˜ êµ¬ê°„ìœ¼ë¡œ ë‹¨ê¸° ì¡°ì • ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
            elif rsi_v <= 30:
                lines.append(f"**RSI {rsi_v:.1f}**: ê³¼ë§¤ë„ êµ¬ê°„ìœ¼ë¡œ ë°˜ë“± ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                lines.append(f"**RSI {rsi_v:.1f}**: ì¤‘ë¦½ êµ¬ê°„ìœ¼ë¡œ ì¶”ì„¸ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # MACD ë¶„ì„
        if pd.notna(macd_line.iloc[-1]) and pd.notna(signal_line.iloc[-1]):
            if macd_line.iloc[-1] > signal_line.iloc[-1]:
                lines.append("**MACDê°€ ì‹œê·¸ë„ì„  ìœ„**ì— ìˆì–´ ìƒìŠ¹ ëª¨ë©˜í…€ì´ ê°ì§€ë©ë‹ˆë‹¤.")
            else:
                lines.append("**MACDê°€ ì‹œê·¸ë„ì„  ì•„ë˜**ì— ìˆì–´ í•˜ë½/ì¡°ì • ëª¨ë©˜í…€ì…ë‹ˆë‹¤.")
        
        # ì§€ì§€/ì €í•­
        dist_sup = (price - support) / support * 100 if support else None
        dist_res = (resistance - price) / price * 100 if price else None
        if dist_sup is not None and dist_res is not None:
            lines.append(f"í˜„ì¬ê°€ ${price:.2f}ëŠ” ì§€ì§€ì„ (${support:.2f})ìœ¼ë¡œë¶€í„° {dist_sup:.1f}%, ì €í•­ì„ (${resistance:.2f})ê¹Œì§€ {dist_res:.1f}% ê±°ë¦¬ì…ë‹ˆë‹¤.")
        
        tech_text = "### ê¸°ìˆ ì  ë¶„ì„\n\n" + "\n\n".join(lines)
        
    except Exception as e:
        print(f"âš ï¸  Technical analysis failed: {e}")
        tech_text = "### ê¸°ìˆ ì  ë¶„ì„\n\në°ì´í„° ì œí•œìœ¼ë¡œ ì°¨íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
    
    state["sections"]["tech"] = tech_text
    state["evals"].append({
        "agent": "TECHNICAL",
        **_binary_eval(tech_text, ["ì°¨íŠ¸", "ì§€í‘œ"], state["sources"]),
        "retries": 0
    })
    
    print(f"    âœ… TECHNICAL completed")
    return state
